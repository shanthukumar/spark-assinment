{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark lab assigniment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxyKGbHU-UyN",
        "outputId": "f429f83f-ab0c-46fc-f669-c4e01afe140b"
      },
      "source": [
        "!apt-get update\r\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\r\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rIgn:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,140 kB]\n",
            "Fetched 2,392 kB in 3s (877 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDKy9VcK-lGO"
      },
      "source": [
        "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\r\n",
        "!pip install -q findspark\r\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHE5A_UE-tfy",
        "outputId": "d1ff6e51-2862-4261-bd07-89248a506f0f"
      },
      "source": [
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\"\r\n",
        "\r\n",
        "!ls\r\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "citizen.txt\t\t       spark-2.3.1-bin-hadoop2.7.tgz.1\n",
            "number.txt\t\t       spark-2.3.1-bin-hadoop2.7.tgz.2\n",
            "sample_data\t\t       spark-warehouse\n",
            "spark-2.3.1-bin-hadoop2.7      state_codes_of_different_states.txt\n",
            "spark-2.3.1-bin-hadoop2.7.tgz  textfile.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSQRqVSq-xCl"
      },
      "source": [
        "import findspark\r\n",
        "findspark.init()\r\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "aSuB7KaC-z5v",
        "outputId": "bad4ada4-123a-473a-9bfd-0efc64eb4ca5"
      },
      "source": [
        "import pyspark\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "spark = SparkSession.builder.appName(\"application\").getOrCreate() \r\n",
        "spark"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7a37e32880f0:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>application</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f936d9b1240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GicKNQob-6Bg",
        "outputId": "c9b581d7-df3e-4160-d4b2-f8fe430b5e81"
      },
      "source": [
        "df = spark.createDataFrame([{\"spark on\": \"colab\"} for x in range(1000)])\r\n",
        "df.show(1)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/session.py:340: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+--------+\n",
            "|spark on|\n",
            "+--------+\n",
            "|   colab|\n",
            "+--------+\n",
            "only showing top 1 row\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_ahY9gOBm4g"
      },
      "source": [
        "#1.\tCreate RDDs in three different ways."
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XZgTSPA-2S9",
        "outputId": "82621b6e-c860-4076-f5b9-994af4634e05"
      },
      "source": [
        "#1st method\r\n",
        "text=\"List Of Popular OTT Apps In India. Spuul, SonyLIV\t,TVFPlay ,Netflix.\".split(\" \")\r\n",
        "words = spark.sparkContext.parallelize(text,2)\r\n",
        "type(words)\r\n",
        "words.collect()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['List',\n",
              " 'Of',\n",
              " 'Popular',\n",
              " 'OTT',\n",
              " 'Apps',\n",
              " 'In',\n",
              " 'India.',\n",
              " 'Spuul,',\n",
              " 'SonyLIV\\t,TVFPlay',\n",
              " ',Netflix.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE4U-ci6_J0w",
        "outputId": "dcb1bff4-8c3d-4ff5-f937-71edca213469"
      },
      "source": [
        "#2nd method\r\n",
        "rdd_2=spark.sparkContext.textFile(\"textfile.txt\")\r\n",
        "rdd_2.collect()\r\n",
        "type(rdd_2)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDvs54Ah_Xru",
        "outputId": "5523219a-f904-4d62-f5a3-00301afc2e5c"
      },
      "source": [
        "#3rd method\r\n",
        "rdd_3=words.filter(lambda word:word.startswith('s'))\r\n",
        "rdd_3.collect()\r\n",
        "type(rdd_3)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.PipelinedRDD"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLBqT403_fBf"
      },
      "source": [
        "#2.Read a text file and count the number of words in the file using RDD operations."
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM5Et6j3_pmH",
        "outputId": "a1b58b09-c51f-4809-b0a0-acc4d3d48b90"
      },
      "source": [
        "word_count=rdd_2.flatMap(lambda s:s.split(\" \"))\r\n",
        "word_count.count()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeBRP2HO_r4f"
      },
      "source": [
        "#3.Write a program to find the word frequency in a given file."
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZWYrMDy_vWX",
        "outputId": "dcc31323-7b91-4465-b070-a51ece61450f"
      },
      "source": [
        "info=spark.sparkContext.textFile(\"textfile.txt\")\r\n",
        "info=info.flatMap(lambda x:x.split())\r\n",
        "content_map=info.map(lambda c:(c,1))\r\n",
        "content_map.reduceByKey(lambda a,b:a+b).collect()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('List', 1),\n",
              " ('Of', 1),\n",
              " ('Spuul,', 1),\n",
              " (',TVFPlay', 1),\n",
              " ('Popular', 1),\n",
              " ('OTT', 1),\n",
              " ('Apps', 1),\n",
              " ('In', 1),\n",
              " ('India.', 1),\n",
              " ('SonyLIV', 1),\n",
              " (',Netflix.', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoNbm07z_yhT"
      },
      "source": [
        "#4.Write a program to convert all words in a file to uppercase."
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2Du3yoSABgV",
        "outputId": "4d8e233a-f3e3-4b54-a263-6569321500da"
      },
      "source": [
        "rdd_2.map(lambda c:c.upper()).collect()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['LIST OF POPULAR OTT APPS IN INDIA.', 'SPUUL, SONYLIV\\t,TVFPLAY ,NETFLIX.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKX2p9LLAcBD"
      },
      "source": [
        "5.Write a program to convert all words in a file to lowercase.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhAIVGv_AEYU"
      },
      "source": [
        "#5.Write a program to convert all words in a file to lowercase."
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFSoWm11AHZz",
        "outputId": "9c198c5c-8bbd-4434-f9ae-047a659127c8"
      },
      "source": [
        "rdd_2.map(lambda c:c.lower()).collect()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['list of popular ott apps in india.', 'spuul, sonyliv\\t,tvfplay ,netflix.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtmlkoIoARSa"
      },
      "source": [
        "6.Write a program to capitalize first letter of each words in file (use string capitalize() method).\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Mq-Kf47tAYJk",
        "outputId": "a428ce69-f1ea-4542-9c51-c3116a773f72"
      },
      "source": [
        "capital=rdd_2.flatMap(lambda a:a.split(\" \")).map(lambda c:c.capitalize()).collect()\r\n",
        "\" \".join(capital)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'List Of Popular Ott Apps In India. Spuul, Sonyliv\\t,tvfplay ,netflix.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rQRHLp1AqWL"
      },
      "source": [
        "7.Find the longest length of word from given set of words.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pxg81iQrAuJY",
        "outputId": "e1ee96af-5750-4511-c728-2c9bad339cce"
      },
      "source": [
        "longest_word=rdd_2.flatMap(lambda x:x.split(\" \"))\r\n",
        "longest_word.map(lambda nu:(len(nu),nu)).max()[1]"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'SonyLIV\\t,TVFPlay'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4tmcrz_AyyR"
      },
      "source": [
        "8.Map the Registration numbers to corresponding branch. 6000 series BDA, 9000 series HAD, 1000 series MS, 2000 series VLSI, 3000 series ES, 4000 series MSc, 5000 series CC. Given registration number, generate a key-value pair of Registration Number and Corresponding Branch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7kPRGxCBDvv",
        "outputId": "d13e8031-ed84-48f3-e80a-5da9d5497b38"
      },
      "source": [
        "registration_number=[6027,2005,2035,6011,9007,9056,3088,3045,4088,4065,5077,5066,1001,1002]\r\n",
        "context=spark.sparkContext.parallelize(registration_number,2)\r\n",
        "classify=context.map(lambda reg:('VLSI',reg) if reg>2000 and reg<3000 \r\n",
        "        else ('MS',reg) if reg>1000 and reg<2000\r\n",
        "        else ('ES',reg) if reg>3000 and reg<4000\r\n",
        "        else ('MSc',reg) if reg>4000 and reg<5000\r\n",
        "        else ('CC',reg) if reg>5000 and reg<6000\r\n",
        "        else ('BDA',reg) if reg>6000 and reg<7000\r\n",
        "        else ('HDA',reg))\r\n",
        "classify.collect()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('BDA', 6027),\n",
              " ('VLSI', 2005),\n",
              " ('VLSI', 2035),\n",
              " ('BDA', 6011),\n",
              " ('HDA', 9007),\n",
              " ('HDA', 9056),\n",
              " ('ES', 3088),\n",
              " ('ES', 3045),\n",
              " ('MSc', 4088),\n",
              " ('MSc', 4065),\n",
              " ('CC', 5077),\n",
              " ('CC', 5066),\n",
              " ('MS', 1001),\n",
              " ('MS', 1002)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9emnLsbBIex"
      },
      "source": [
        "9.Text file contain numbers. Numbers are separated by one white space. \r\n",
        "There is no order to store the numbers. One line may contain one or more numbers.\r\n",
        "Find the maximum, minimum, sum and mean of numbers.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-IQvZ-1Bsgd",
        "outputId": "3f19390d-6270-4340-b2c7-349c995bbfb5"
      },
      "source": [
        "file1=spark.sparkContext.textFile(\"number.txt\")\r\n",
        "file1_rdd=file1.flatMap(lambda z:z.split(\" \")).map(lambda c:int(c))\r\n",
        "file1_rdd.max()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "217"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeS7X3AxBzhD",
        "outputId": "f74b97b0-e847-4ddd-df82-24374d0f5446"
      },
      "source": [
        "file1_rdd.min()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8bUn82PB1b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b20987b-fac2-41ca-ef72-71fdb7c05953"
      },
      "source": [
        "file1_rdd.sum()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1806"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOtAMqhlB32n",
        "outputId": "86a9cfb4-d1ed-43bd-f90e-b5df2eb4e510"
      },
      "source": [
        "file1_rdd.mean()"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWiWDrAnB9iI"
      },
      "source": [
        "10. A text file (citizen.txt) contains data about citizens of country. Fields (information in file) are Name, dob, Phone, email and state name. Another file contains mapping of state names to state code like Karnataka is codes as KA, TamilNadu as TN, Kerala KL etc. Compress the citizen.txt file by changing full state name to state code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oywCA-pCCdQ",
        "outputId": "3e570009-a643-4542-98ca-19e833a5585f"
      },
      "source": [
        "details=spark.sparkContext.textFile(\"citizen.txt\")\r\n",
        "code=spark.sparkContext.textFile(\"state_codes_of_different_states.txt\")\r\n",
        "details_rdd=details.map(lambda x:x.split(\",\")).collect()\r\n",
        "code_rdd=code.map(lambda y:y.split(\",\")).collect()\r\n",
        "for i in range(len(details_rdd)):\r\n",
        "    for j in range(len(code_rdd)):  \r\n",
        "        if details_rdd[i][4]==code_rdd[j][0]:\r\n",
        "            details_rdd[i][4]=code_rdd[j][1]\r\n",
        "details_rdd              "
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['shanth kumar b', '18-08-1998', '86573', 'shanth656@gmail.com', 'KA'],\n",
              " ['manish', '01-02-1998', '98786', 'yashwanth@gmail.com', 'JK'],\n",
              " ['divya', '31-01-1996', '98675', 'sharanya@gmail.com', 'AP'],\n",
              " ['Chethana', '12-03-1995', '89523', 'chethana@gmail.com', 'TN'],\n",
              " ['Akshith', '04-07-1993', '78234', 'akshith@gmail.com', 'GJ'],\n",
              " ['Amruta', '21-10-1996', '65432', 'amruta@gmail.com', 'WB'],\n",
              " ['Anupam', '05-09-1993', '87474', 'anupam@gmail.com', 'RJ'],\n",
              " ['Chandan', '13-02-1997', '76893', 'chandan@gmail.com', 'PB'],\n",
              " ['Himana', '04-08-1995', '98234', 'himana@gmail.com', 'JK'],\n",
              " ['Varsha', '05-09-1997', '86574', 'varsha@gmail.com', 'MH'],\n",
              " ['Arun', '17-12-1994', '67895', 'arun@gmail.com', 'UP']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7mCUAnmL1Lr"
      },
      "source": [
        "stRDD = spark.sparkContext.textFile('state_codes_of_different_states.txt')\r\n",
        "stateKey = stRDD.map(lambda word: (word.split(',')[0], word.split(',')[1]))"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8QOH-x-MeAk",
        "outputId": "6e6a2388-47d3-4280-dad3-84c6984ce796"
      },
      "source": [
        "#creating dictionary\r\n",
        "code_dict = {}\r\n",
        "for val in stateKey.collect():\r\n",
        "    code_dict[val[0]] = val[1]\r\n",
        "    \r\n",
        "code_dict"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AndhraPradesh': 'AP',\n",
              " 'Gujarat': 'GJ',\n",
              " 'JammuKashmir': 'JK',\n",
              " 'Karnataka': 'KA',\n",
              " 'Maharashtra': 'MH',\n",
              " 'Punjab': 'PB',\n",
              " 'Rajasthan': 'RJ',\n",
              " 'TamilNadu': 'TN',\n",
              " 'UttarPradesh': 'UP',\n",
              " 'WestBengal': 'WB'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sz5rg7YEMlYd",
        "outputId": "5c56a626-e06d-407d-e5ce-4f29901dc2f5"
      },
      "source": [
        "mapData = spark.sparkContext.broadcast(code_dict)\r\n",
        "\r\n",
        "cityRdd = spark.sparkContext.textFile('citizen.txt')\r\n",
        "print(cityRdd.collect())\r\n",
        "def abc(state,codes):\r\n",
        "    splitData = state.split(',')  \r\n",
        "    print(splitData)\r\n",
        "    splitData[4] = codes.value.get(splitData[4])\r\n",
        "    newData = ' '\r\n",
        "    newData = newData.join(splitData)\r\n",
        "    \r\n",
        "    return newData\r\n",
        "    \r\n",
        "mapCitizen = cityRdd.map(lambda data: abc(data,mapData))\r\n",
        "mapCitizen.collect()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['shanth kumar b,18-08-1998,86573,shanth656@gmail.com,Karnataka', 'manish,01-02-1998,98786,yashwanth@gmail.com,JammuKashmir', 'divya,31-01-1996,98675,sharanya@gmail.com,AndhraPradesh', 'Chethana,12-03-1995,89523,chethana@gmail.com,TamilNadu', 'Akshith,04-07-1993,78234,akshith@gmail.com,Gujarat', 'Amruta,21-10-1996,65432,amruta@gmail.com,WestBengal', 'Anupam,05-09-1993,87474,anupam@gmail.com,Rajasthan', 'Chandan,13-02-1997,76893,chandan@gmail.com,Punjab', 'Himana,04-08-1995,98234,himana@gmail.com,JammuKashmir', 'Varsha,05-09-1997,86574,varsha@gmail.com,Maharashtra', 'Arun,17-12-1994,67895,arun@gmail.com,UttarPradesh']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['shanth kumar b 18-08-1998 86573 shanth656@gmail.com KA',\n",
              " 'manish 01-02-1998 98786 yashwanth@gmail.com JK',\n",
              " 'divya 31-01-1996 98675 sharanya@gmail.com AP',\n",
              " 'Chethana 12-03-1995 89523 chethana@gmail.com TN',\n",
              " 'Akshith 04-07-1993 78234 akshith@gmail.com GJ',\n",
              " 'Amruta 21-10-1996 65432 amruta@gmail.com WB',\n",
              " 'Anupam 05-09-1993 87474 anupam@gmail.com RJ',\n",
              " 'Chandan 13-02-1997 76893 chandan@gmail.com PB',\n",
              " 'Himana 04-08-1995 98234 himana@gmail.com JK',\n",
              " 'Varsha 05-09-1997 86574 varsha@gmail.com MH',\n",
              " 'Arun 17-12-1994 67895 arun@gmail.com UP']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    }
  ]
}